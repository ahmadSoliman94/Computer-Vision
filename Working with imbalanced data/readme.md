### Imbalanced Data for images set:
#### means that the distribution of classes (categories) within the dataset is uneven. In other words, some classes have a significantly larger number of samples (images) than others. For example, consider a dataset of images of animals, where there are 1,000 images of cats but only 50 images of tigers. This dataset would be imbalanced with respect to the classes "cat" and "tiger."

### Techniques for handling imbalanced data:
#### 1. Random forest: 
#### Pick decision tree based approaches as they work better than logistic regression or SVM. Random Forest is a good algorithm to try but beware of overfitting. 
#### 2. Up-sampling: is a common technique used to address class imbalance in a dataset. The idea is to artificially increase the number of samples in the minority class to balance out the distribution of classes. In the context of image data, this is often done through various types of data augmentation. For example, if we have a dataset of images of cats and dogs, we can up-sample the images of dogs by applying random transformations such as rotation, translation, scaling, and flipping. This will increase the number of images of dogs in the dataset, thereby balancing out the distribution of classes.
#### 3. Down-sampling: is the opposite of up-sampling. Instead of increasing the number of samples in the minority class, we decrease the number of samples in the majority class. This is typically done by randomly removing samples from the majority class. For example, if we have a dataset of images of cats and dogs, we can down-sample the images of cats by randomly removing images from the dataset. This will decrease the number of images of cats in the dataset, thereby balancing out the distribution of classes.

#### 4. A combination of Over and under sampling: Combining over-sampling of the minority class and under-sampling of the majority class can be an effective strategy for handling class imbalance. This approach aims to balance the class distribution by both increasing the occurrence of the minority class and decreasing the occurrence of the majority class. The goal is to improve the model's performance across all classes without compromising its ability to generalize to new data. For example, if we have a dataset of images of cats and dogs, we can up-sample the images of dogs and down-sample the images of cats. This will increase the number of images of dogs in the dataset and decrease the number of images of cats in the dataset, thereby balancing out the distribution of classes.

#### 5. Penalize learning algorithms (Cost-Sensitive Training): Another approach to handling class imbalance is to use learning algorithms that can penalize misclassification of the minority class more than misclassification of the majority class. This approach is known as cost-sensitive learning. The idea is to make the learning algorithm more sensitive to the minority class by increasing the cost of misclassification for the minority class. For example, if we have a dataset of images of cats and dogs, we can increase the cost of misclassifying a dog as a cat. This will make the learning algorithm more sensitive to the minority class (dogs) and less sensitive to the majority class (cats), thereby balancing out the distribution of classes.

#### 6. Generate synthetic samples: Synthetic Minority Oversampling Technique (SMOTE) is a popular algorithm for generating synthetic samples. The idea is to generate synthetic samples from the minority class rather than creating copies of existing samples from the minority class. This approach has been shown to be effective in improving the performance of learning algorithms on imbalanced datasets. For example, if we have a dataset of images of cats and dogs, we can generate synthetic samples of dogs by applying random transformations such as rotation, translation, scaling, and flipping to existing images of dogs. This will increase the number of images of dogs in the dataset, thereby balancing out the distribution of classes.


#### 7. Add appropriate weights to your deep learning model. This is a very simple and effective technique. You can add weights to your loss function to make your model more sensitive to minority classes. For example, if we have a dataset of images of cats and dogs, we can add weights to the loss function to make the model more sensitive to images of dogs. This will make the model more sensitive to the minority class (dogs) and less sensitive to the majority class (cats), thereby balancing out the distribution of classes.